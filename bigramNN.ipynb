{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aca358fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53982"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = [w.lower() for w in open('Indian_Names.txt','r').read().splitlines()]\n",
    "words[:5]\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2d5b9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars =  sorted(list(set(''.join(words))))\n",
    "stoi = {s:i for i,s in enumerate(chars)}\n",
    "stoi['<S>'] = 26\n",
    "stoi['<E>'] = 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62e41aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<S> a\n",
      "a a\n",
      "a b\n",
      "b a\n",
      "a n\n",
      "n <E>\n",
      "tensor([26,  0,  0,  1,  0, 13])\n",
      "tensor([ 0,  0,  1,  0, 13, 27])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "xs,ys = [],[]\n",
    "for w in words[:1] :\n",
    "    chr = ['<S>'] + list(w) + ['<E>']\n",
    "    for c1,c2 in zip(chr,chr[1:]):\n",
    "        print(c1,c2)\n",
    "        xs.append(stoi[c1])\n",
    "        ys.append(stoi[c2])\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "print(xs)\n",
    "print(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b87fa10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 28]) torch.Size([6, 28])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "xenc,yenc = F.one_hot(xs, num_classes=28).float(), F.one_hot(ys, num_classes=28).float()\n",
    "print(xenc.shape, yenc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98a00b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0046, 0.0269, 0.0468, 0.0482, 0.0178, 0.0108, 0.0315, 0.0058, 0.0280,\n",
      "         0.0036, 0.0150, 0.0070, 0.0742, 0.0141, 0.1679, 0.0028, 0.0059, 0.0127,\n",
      "         0.0295, 0.1020, 0.0323, 0.1004, 0.0063, 0.0888, 0.0437, 0.0236, 0.0210,\n",
      "         0.0289],\n",
      "        [0.0334, 0.0471, 0.0290, 0.1841, 0.0045, 0.0047, 0.0160, 0.0346, 0.1300,\n",
      "         0.0082, 0.0081, 0.0159, 0.0054, 0.0068, 0.0053, 0.0034, 0.0178, 0.0101,\n",
      "         0.0373, 0.0184, 0.0180, 0.0191, 0.0322, 0.0641, 0.1299, 0.0221, 0.0597,\n",
      "         0.0347],\n",
      "        [0.0334, 0.0471, 0.0290, 0.1841, 0.0045, 0.0047, 0.0160, 0.0346, 0.1300,\n",
      "         0.0082, 0.0081, 0.0159, 0.0054, 0.0068, 0.0053, 0.0034, 0.0178, 0.0101,\n",
      "         0.0373, 0.0184, 0.0180, 0.0191, 0.0322, 0.0641, 0.1299, 0.0221, 0.0597,\n",
      "         0.0347],\n",
      "        [0.0179, 0.0228, 0.0862, 0.0049, 0.0387, 0.0049, 0.0058, 0.0136, 0.0872,\n",
      "         0.0183, 0.1345, 0.0399, 0.0126, 0.0070, 0.0105, 0.1443, 0.0084, 0.0993,\n",
      "         0.0240, 0.0219, 0.0466, 0.0091, 0.0142, 0.0063, 0.0159, 0.0409, 0.0256,\n",
      "         0.0389],\n",
      "        [0.0334, 0.0471, 0.0290, 0.1841, 0.0045, 0.0047, 0.0160, 0.0346, 0.1300,\n",
      "         0.0082, 0.0081, 0.0159, 0.0054, 0.0068, 0.0053, 0.0034, 0.0178, 0.0101,\n",
      "         0.0373, 0.0184, 0.0180, 0.0191, 0.0322, 0.0641, 0.1299, 0.0221, 0.0597,\n",
      "         0.0347],\n",
      "        [0.0597, 0.0167, 0.0240, 0.0293, 0.0362, 0.0745, 0.0462, 0.0017, 0.0717,\n",
      "         0.0502, 0.0563, 0.0177, 0.0103, 0.0717, 0.0302, 0.0440, 0.0354, 0.0597,\n",
      "         0.0250, 0.0344, 0.0347, 0.0168, 0.0345, 0.0197, 0.0148, 0.0673, 0.0115,\n",
      "         0.0059]])\n"
     ]
    }
   ],
   "source": [
    "gen = torch.Generator().manual_seed(21474836478) \n",
    "W =  torch.randn((28, 28))  # weight matrix\n",
    "counts = (xenc @ W).exp()\n",
    "prob =  counts / counts.sum(1, keepdim=True)\n",
    "print(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "baaf80c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob[0].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1df521",
   "metadata": {},
   "source": [
    "Generating the nll of a word with the randomized weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cc1f4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigram example 1: <S> -> a (indexes 26,0)\n",
      "input to the neural network: 26\n",
      "actual next character: a\n",
      "probability of the next character: 0.004594045225530863\n",
      "negative log likelihood: 5.382994174957275\n",
      "----------\n",
      "bigram example 2: a -> a (indexes 0,0)\n",
      "input to the neural network: 0\n",
      "actual next character: a\n",
      "probability of the next character: 0.033369746059179306\n",
      "negative log likelihood: 3.4001054763793945\n",
      "----------\n",
      "bigram example 3: a -> b (indexes 0,1)\n",
      "input to the neural network: 0\n",
      "actual next character: b\n",
      "probability of the next character: 0.04713344946503639\n",
      "negative log likelihood: 3.05477237701416\n",
      "----------\n",
      "bigram example 4: b -> a (indexes 1,0)\n",
      "input to the neural network: 1\n",
      "actual next character: a\n",
      "probability of the next character: 0.01789383962750435\n",
      "negative log likelihood: 4.023298740386963\n",
      "----------\n",
      "bigram example 5: a -> n (indexes 0,13)\n",
      "input to the neural network: 0\n",
      "actual next character: n\n",
      "probability of the next character: 0.006839754991233349\n",
      "negative log likelihood: 4.985003471374512\n",
      "----------\n",
      "bigram example 6: n -> <E> (indexes 13,27)\n",
      "input to the neural network: 13\n",
      "actual next character: <E>\n",
      "probability of the next character: 0.005928130354732275\n",
      "negative log likelihood: 5.12804651260376\n",
      "----------\n",
      "average negative log likelihood: 4.329036235809326\n"
     ]
    }
   ],
   "source": [
    "itos = {i:s for s,i in stoi.items()}\n",
    "nll = []\n",
    "for i in range(xs.shape[0]):\n",
    "    x = xs[i].item()\n",
    "    y = ys[i].item()\n",
    "    print(f'bigram example {i + 1}: {itos[x]} -> {itos[y]} (indexes {x},{y})')\n",
    "    print('input to the neural network:', x)\n",
    "    #print('output of the neural network:', prob[i])\n",
    "    print('actual next character:', itos[y])\n",
    "    print('probability of the next character:', prob[i, y].item())\n",
    "    nll_value = -torch.log(prob[i, y])\n",
    "    nll.append(nll_value)\n",
    "    print('negative log likelihood:', nll_value.item())\n",
    "    print('----------')\n",
    "nll = torch.stack(nll)\n",
    "print('average negative log likelihood:', nll.mean().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5d1143",
   "metadata": {},
   "source": [
    "In the above output the loss was very high with randomnized weights without trainign the NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c16f08e",
   "metadata": {},
   "source": [
    "Designing a single layer Neural Net with softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "045b6b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.3290)\n"
     ]
    }
   ],
   "source": [
    "# representing the loss in tensor\n",
    "loss = -prob[torch.arange(6), ys].log().mean()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6729f53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = torch.Generator().manual_seed(21474836478)\n",
    "W = torch.randn((28, 28), generator=gen,requires_grad= True)  # weight matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bb3caab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.8174, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#forward pass\n",
    "\n",
    "xenc = F.one_hot(xs, num_classes=28).float()\n",
    "logits = xenc @ W\n",
    "counts = logits.exp()\n",
    "prob = counts / counts.sum(1, keepdim=True)\n",
    "loss = -prob[torch.arange(xs.shape[0]), ys].log().mean()\n",
    "print(loss) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56e2df19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.8174, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# representing the loss in tensor\n",
    "loss = -prob[torch.arange(xs.shape[0]), ys].log().mean()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e46c002a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#backward pass\n",
    "W.grad = None\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1600c330",
   "metadata": {},
   "outputs": [],
   "source": [
    "#W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff3266d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "W.data += -0.1 * W.grad\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c842e8ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of examples: 488074\n"
     ]
    }
   ],
   "source": [
    "xs,ys = [],[]\n",
    "for w in words:\n",
    "    chr = ['<S>'] + list(w) + ['<E>']\n",
    "    for c1,c2 in zip(chr,chr[1:]):\n",
    "        xs.append(stoi[c1])\n",
    "        ys.append(stoi[c2])\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "print('no. of examples:', xs.shape[0])\n",
    "g = torch.Generator().manual_seed(21474836478)\n",
    "W = torch.randn((28, 28), generator=g, requires_grad=True)  #weight matrix           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb36d56",
   "metadata": {},
   "source": [
    "Training the neural network with gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f99db3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6135923862457275\n",
      "3.3724989891052246\n",
      "3.176208972930908\n",
      "3.0262224674224854\n",
      "2.9123330116271973\n",
      "2.825191020965576\n",
      "2.758117437362671\n",
      "2.7050046920776367\n",
      "2.661327838897705\n",
      "2.624281406402588\n",
      "2.592236042022705\n",
      "2.5642154216766357\n",
      "2.5395736694335938\n",
      "2.517829418182373\n",
      "2.4985861778259277\n",
      "2.4814960956573486\n",
      "2.466247081756592\n",
      "2.4525644779205322\n",
      "2.440211772918701\n",
      "2.428990125656128\n",
      "2.418738603591919\n",
      "2.4093258380889893\n",
      "2.4006450176239014\n",
      "2.392608165740967\n",
      "2.3851423263549805\n",
      "2.378185272216797\n",
      "2.37168288230896\n",
      "2.3655893802642822\n",
      "2.359863758087158\n",
      "2.354470729827881\n",
      "2.349379301071167\n",
      "2.34456205368042\n",
      "2.3399951457977295\n",
      "2.3356566429138184\n",
      "2.3315281867980957\n",
      "2.3275933265686035\n",
      "2.3238375186920166\n",
      "2.3202476501464844\n",
      "2.3168118000030518\n",
      "2.3135204315185547\n",
      "2.310363531112671\n",
      "2.30733323097229\n",
      "2.304421901702881\n",
      "2.3016226291656494\n",
      "2.298928737640381\n",
      "2.2963356971740723\n",
      "2.293837547302246\n",
      "2.2914295196533203\n",
      "2.28910756111145\n",
      "2.2868669033050537\n",
      "2.2847044467926025\n",
      "2.282615900039673\n",
      "2.2805988788604736\n",
      "2.2786495685577393\n",
      "2.2767651081085205\n",
      "2.2749431133270264\n",
      "2.2731807231903076\n",
      "2.271475076675415\n",
      "2.269824266433716\n",
      "2.26822566986084\n",
      "2.2666776180267334\n",
      "2.2651777267456055\n",
      "2.2637240886688232\n",
      "2.262315034866333\n",
      "2.260948657989502\n",
      "2.2596232891082764\n",
      "2.2583372592926025\n",
      "2.257089138031006\n",
      "2.2558772563934326\n",
      "2.2547006607055664\n",
      "2.2535572052001953\n",
      "2.252446413040161\n",
      "2.251366376876831\n",
      "2.2503161430358887\n",
      "2.249295234680176\n",
      "2.2483012676239014\n",
      "2.2473342418670654\n",
      "2.2463927268981934\n",
      "2.2454757690429688\n",
      "2.2445831298828125\n",
      "2.2437124252319336\n",
      "2.2428643703460693\n",
      "2.242037534713745\n",
      "2.2412312030792236\n",
      "2.2404444217681885\n",
      "2.2396764755249023\n",
      "2.238927125930786\n",
      "2.2381954193115234\n",
      "2.237480878829956\n",
      "2.2367827892303467\n",
      "2.236100673675537\n",
      "2.235434055328369\n",
      "2.2347822189331055\n",
      "2.2341442108154297\n",
      "2.233520984649658\n",
      "2.2329108715057373\n",
      "2.232313871383667\n",
      "2.23172926902771\n",
      "2.231156587600708\n",
      "2.2305960655212402\n",
      "2.2300467491149902\n",
      "2.229508638381958\n",
      "2.2289812564849854\n",
      "2.228464126586914\n",
      "2.227957010269165\n",
      "2.227459669113159\n",
      "2.2269716262817383\n",
      "2.2264931201934814\n",
      "2.2260236740112305\n",
      "2.225562334060669\n",
      "2.225109815597534\n",
      "2.2246649265289307\n",
      "2.2242283821105957\n",
      "2.223799228668213\n",
      "2.2233779430389404\n",
      "2.222963809967041\n",
      "2.2225565910339355\n",
      "2.222156286239624\n",
      "2.2217628955841064\n",
      "2.2213759422302246\n",
      "2.2209951877593994\n",
      "2.220620632171631\n",
      "2.22025203704834\n",
      "2.2198898792266846\n",
      "2.2195324897766113\n",
      "2.2191812992095947\n",
      "2.2188353538513184\n",
      "2.2184948921203613\n",
      "2.2181591987609863\n",
      "2.2178287506103516\n",
      "2.217503309249878\n",
      "2.2171826362609863\n",
      "2.2168664932250977\n",
      "2.216555118560791\n",
      "2.216248035430908\n",
      "2.215945243835449\n",
      "2.2156472206115723\n",
      "2.215353012084961\n",
      "2.2150628566741943\n",
      "2.2147765159606934\n",
      "2.2144947052001953\n",
      "2.2142162322998047\n",
      "2.2139413356781006\n",
      "2.213670492172241\n",
      "2.2134029865264893\n",
      "2.213139295578003\n",
      "2.212878465652466\n",
      "2.2126214504241943\n",
      "2.2123680114746094\n",
      "2.2121171951293945\n",
      "2.211869716644287\n",
      "2.211625337600708\n",
      "2.211383819580078\n",
      "2.2111454010009766\n",
      "2.2109103202819824\n",
      "2.2106778621673584\n",
      "2.2104477882385254\n",
      "2.2102210521698\n",
      "2.2099969387054443\n",
      "2.209774971008301\n",
      "2.2095561027526855\n",
      "2.2093396186828613\n",
      "2.209125518798828\n",
      "2.208914279937744\n",
      "2.208705186843872\n",
      "2.20849871635437\n",
      "2.208294153213501\n",
      "2.208092451095581\n",
      "2.207892656326294\n",
      "2.207695245742798\n",
      "2.2074997425079346\n",
      "2.2073066234588623\n",
      "2.207115411758423\n",
      "2.2069265842437744\n",
      "2.206739664077759\n",
      "2.206554651260376\n",
      "2.206371784210205\n",
      "2.206190824508667\n",
      "2.2060117721557617\n",
      "2.2058346271514893\n",
      "2.2056593894958496\n",
      "2.2054858207702637\n",
      "2.2053139209747314\n",
      "2.205144166946411\n",
      "2.2049758434295654\n",
      "2.2048094272613525\n",
      "2.2046449184417725\n",
      "2.204481840133667\n",
      "2.204320192337036\n",
      "2.204160213470459\n",
      "2.2040023803710938\n",
      "2.203845262527466\n",
      "2.2036900520324707\n",
      "2.2035365104675293\n",
      "2.2033843994140625\n",
      "2.2032339572906494\n",
      "2.203084707260132\n",
      "2.202936887741089\n",
      "2.2027907371520996\n",
      "2.202645778656006\n",
      "2.202502489089966\n",
      "2.202360153198242\n",
      "2.202219009399414\n",
      "2.2020797729492188\n",
      "2.201941728591919\n",
      "2.2018043994903564\n",
      "2.201669216156006\n",
      "2.2015345096588135\n",
      "2.201401948928833\n",
      "2.2012696266174316\n",
      "2.201138734817505\n",
      "2.201009511947632\n",
      "2.200881004333496\n",
      "2.200753927230835\n",
      "2.2006278038024902\n",
      "2.200502872467041\n",
      "2.200378894805908\n",
      "2.20025634765625\n",
      "2.200134754180908\n",
      "2.200014352798462\n",
      "2.199894666671753\n",
      "2.1997764110565186\n",
      "2.1996588706970215\n",
      "2.19954252243042\n",
      "2.1994271278381348\n",
      "2.199312686920166\n",
      "2.199199676513672\n",
      "2.199086904525757\n",
      "2.1989758014678955\n",
      "2.1988651752471924\n",
      "2.1987555027008057\n",
      "2.1986467838287354\n",
      "2.1985392570495605\n",
      "2.198432207107544\n",
      "2.1983261108398438\n",
      "2.198221206665039\n",
      "2.1981170177459717\n",
      "2.1980135440826416\n",
      "2.197911262512207\n",
      "2.1978096961975098\n",
      "2.197709083557129\n",
      "2.1976089477539062\n",
      "2.197510004043579\n",
      "2.1974117755889893\n",
      "2.1973142623901367\n",
      "2.1972174644470215\n",
      "2.1971211433410645\n",
      "2.197026491165161\n",
      "2.196932077407837\n",
      "2.19683837890625\n",
      "2.1967451572418213\n",
      "2.196653127670288\n",
      "2.196561574935913\n",
      "2.1964709758758545\n",
      "2.196381092071533\n",
      "2.19629168510437\n",
      "2.1962029933929443\n",
      "2.196115016937256\n",
      "2.1960277557373047\n",
      "2.19594144821167\n",
      "2.1958556175231934\n",
      "2.195770502090454\n",
      "2.195685863494873\n",
      "2.19560170173645\n",
      "2.195518732070923\n",
      "2.1954360008239746\n",
      "2.1953542232513428\n",
      "2.195272922515869\n",
      "2.195192337036133\n",
      "2.1951117515563965\n",
      "2.1950323581695557\n",
      "2.194953441619873\n",
      "2.1948752403259277\n",
      "2.1947972774505615\n",
      "2.1947202682495117\n",
      "2.19464373588562\n",
      "2.1945676803588867\n",
      "2.1944923400878906\n",
      "2.1944174766540527\n",
      "2.194343328475952\n",
      "2.1942694187164307\n",
      "2.1941959857940674\n",
      "2.1941232681274414\n",
      "2.1940512657165527\n",
      "2.1939797401428223\n",
      "2.19390869140625\n",
      "2.193837881088257\n",
      "2.193767786026001\n",
      "2.1936981678009033\n",
      "2.193629264831543\n",
      "2.1935606002807617\n",
      "2.1934924125671387\n",
      "2.193425178527832\n",
      "2.1933579444885254\n",
      "2.193290948867798\n",
      "2.193225383758545\n",
      "2.193159341812134\n",
      "2.19309401512146\n",
      "2.1930291652679443\n",
      "2.192965030670166\n"
     ]
    }
   ],
   "source": [
    "lr = 20\n",
    "epochs = 300\n",
    "for k in range(epochs):\n",
    "    \n",
    "    xenc = F.one_hot(xs, num_classes=28).float()\n",
    "    logits = xenc @ W\n",
    "    counts = logits.exp()\n",
    "    prob = counts / counts.sum(1, keepdim=True)\n",
    "    loss = -prob[torch.arange(xs.shape[0]), ys].log().mean()\n",
    "    print(loss.item())\n",
    "    #backward passs\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "    #updating the weights after backpropagation\n",
    "    W.data += -lr * W.grad\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c3d80b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mik<E>\n",
      "jlori<E>\n",
      "abamathanaeraya<E>\n",
      "ka<E>\n",
      "savithrnanfrararun<E>\n",
      "s<E>\n",
      "maanek<E>\n",
      "pripun<E>\n",
      "ipcn<E>\n",
      "ishiya<E>\n",
      "kahel<E>\n",
      "kalukhyarullaanth<E>\n",
      "veiraanyanulln<E>\n",
      "rdhukodhak<S>n<E>\n",
      "sniqununi<E>\n",
      "shiandhiniragal<E>\n",
      "kranjwaya<E>\n",
      "rabal<E>\n",
      "n<E>\n",
      "karai<E>\n"
     ]
    }
   ],
   "source": [
    "gen= torch.Generator().manual_seed(21474836478)\n",
    "\n",
    "for i in range(20):\n",
    "    out = []\n",
    "    ix = 26 # start with the first character\n",
    "    while True:\n",
    "        xenc = F.one_hot(torch.tensor([ix]), num_classes=28).float()\n",
    "        logits = xenc @ W\n",
    "        counts = logits.exp()\n",
    "        prob = counts / counts.sum(1, keepdim=True)\n",
    "        ix = torch.multinomial(prob, num_samples=1, generator=gen).item()\n",
    "        out.append(itos[ix])\n",
    "        if ix == stoi['<E>']:\n",
    "            break\n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4a4bd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3456b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
